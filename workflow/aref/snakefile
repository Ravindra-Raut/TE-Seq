rule sym_link:
    input:
        aref_dir = config["symlink_aref"]["aref_symlinkdir"]
    output:
        sym_link_outfile = "aref.symlinked.outfile"
    shell:
        """
ln -s {input.aref_dir} aref
touch {output.sym_link_outfile}
        """

rule sym_link_aref_contents:
    input:
        aref_dir = config["symlink_aref"]["aref_symlinkdir"]
    output:
        sym_link_outfile = "aref_contents.symlinked.outfile"
    shell:
        """
ln -s {input.aref_dir}/* aref/
touch {output.sym_link_outfile}
        """

#tag BASECALLING
rule dorado:
    input:
        dir = "ldna/rawdata/{rate}/{sample}"
    output:
        calls = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modifications_string = "[0-9A-Za-z_-]+"
    params:
        dorado = config["dorado"],
        basecallingModel = lambda w: config["basecallingModel"][w.rate][w.type],
        reference = config["update_ref_with_tldr"]["tldr_input_bam_ref"]
    resources:
        cpus_per_task =12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        slurm_extra="--time=96:00:00 --prefer=a6000 --gres=gpu:2"
    shell:
        """
mkdir -p $(dirname {output.calls})
mod_string=$(echo {wildcards.modifications_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{input.dir} \
--recursive \
--verbose \
--reference {params.reference} > {output.calls}.unfinished
mv {output.calls}.unfinished {output.calls}
        """

rule call_dorado_resume:
    input:
        expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.unfinished", sample = config["samples"], rate = config["rate"], type = config["type"], modifications_string = config["modification_string"])

rule dorado_resume:
    input:
        unfinished = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.unfinished"
    output:
        calls = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam.finished"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modifications_string = "[0-9A-Za-z_-]+"
    params:
        dorado = config["dorado"],
        basecallingModel = lambda w: config["basecallingModel"][w.rate][w.type],
        reference = config["update_ref_with_tldr"]["tldr_input_bam_ref"]
    resources:
        cpus_per_task =12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        slurm_extra="--time=48:00:00 --prefer=a6000 --gres=gpu:2"
    shell:
        """
mkdir -p $(dirname {output.calls})
mod_string=$(echo {wildcards.modifications_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{input.dir} \
--resume-from {input.unfinished} \
--recursive \
--verbose \
--reference {params.reference} > {output.calls}.temp
mv {output.calls}.temp {output.calls}
        """



rule dorado_seqsummary:
    input:
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        sortedbamindex = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam.bai", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    params:
        dorado = config["dorado"]
    output:
        "aref/qc/{sample}/{sample}.doradosummary.txt"
    conda:
        "omics"
    shell:
        """
{params.dorado} summary {input.sortedbam} > {output}
        """

rule pycoQC:
    input:
        seqsummary = "aref/qc/{sample}/{sample}.doradosummary.txt",
        sortedbam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
    output:
        html = "aref/qc/{sample}/{sample}pycoQC.html",
        json = "aref/qc/{sample}/{sample}pycoQC.json"
    resources:
        mem_mb = 100000,
    conda:
        "pycoQC"
    shell:
        """
mkdir -p $(dirname {output})
pycoQC --summary_file {input.seqsummary} --bam_file {input.sortedbam} --html_outfile {output.html} --json_outfile {output.json} --min_pass_qual 10
        """

#tag ALIGNMENT_UTILITIES
rule sortBam:
    input:
        bam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.bam"
    output:
        sortedbam = "aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modifications_string}.sorted.bam"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
mkdir -p $(dirname {output})
samtools sort -@8 -m4g {input.bam} > {output.sortedbam}
        """

rule IndexBam:
    input:
        bam = "aref/{bampath}.bam"
    output:
        index = "aref/{bampath}.bam.bai"
    resources:
        cpus_per_task =10,
        mem_mb = 64000
    conda:
        "omics"
    shell:
        """
samtools index  -@6 {input.bam}
        """

rule tldr_aggregate_multiple_samples:
    input:
        bams = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam", sample = config["samples"], rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        bais = expand("aref/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam.bai", sample = config["samples"], rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    params:
        tldr_input_bam_ref = lambda w: config["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldr_te_ref = config["update_ref_with_tldr"]["tldr_te_ref"]
    output:
        tldr = "aref/A.REF_tldr/A.REF.table.txt"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda:
        "tldr"
    shell:
        """
tldr -b {input.bams} \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--outbase A.REF \
--detail_output \
--extend_consensus 4000 \
--trdcol

mkdir -p $(dirname {output.tldr})
mv A.REF.table.txt {output.tldr}
mv ref $(dirname {output.tldr})/
        """

rule tldr_per_sample:
    input:
        bam = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        bai = expand("aref/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam.bai", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])

    params:
        tldr_input_bam_ref = lambda w: config["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldr_te_ref = config["update_ref_with_tldr"]["tldr_te_ref"]
    output:
        tldr = "aref/{sample}_tldr/{sample}.table.txt"
    resources:
        cpus_per_task = 24,
        mem_mb = 100000,
        runtime = 300
    conda:
        "tldr"
    shell:
        """
tldr -b {input.bam} \
-e {params.tldr_te_ref} \
-r {params.tldr_input_bam_ref} \
-p 20 \
--outbase {wildcards.sample} \
--detail_output \
--extend_consensus 4000 \
--trdcol

mkdir -p $(dirname {output.tldr})
mv {wildcards.sample}.table.txt {output.tldr}
mv {wildcards.sample} $(dirname {output.tldr})/
        """

rule update_reference:
    input:
        reference = lambda w: config["update_ref_with_tldr"]["tldr_input_bam_ref"],
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    output:
        updated_reference = "aref/{sample_or_ref}-pre-ins-filtering.fa",
        non_ref_contigs = "aref/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa"
    conda:
        "repeatanalysis"
    script:
        "scripts/create_reference.R"


rule move_starting_reference:
    params:
        ref = config["starting_ref"]
    output:
        ref = "aref/A.REF.fa"
    shell:
        """
cp {params.ref} {output.ref}
        """

rule get_repeatmasker_raw:
    params:
        repeatmasker = config["starting_ref_repeatmasker"]
    output:
        repeatmasker = "aref/{sample_or_ref}_repeatmasker/A.REF_repeatmasker_raw.gtf"
    shell:
        """
cp {params.repeatmasker} {output.repeatmasker}
        """

rule index_reference:
    input:
        reference = "aref/{sample_or_ref}.fa"
    output:
        reference_index = "aref/{sample_or_ref}.fa.fai"
    conda:
        "omics"
    shell:
        """
samtools faidx {input.reference}
        """


def repeatmasker_input(wildcards):
    if config["update_ref_with_tldr"]["response"] == "yes":
        return expand("aref/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa", sample_or_ref = wildcards.sample_or_ref)
    else:
        return config["ref"]
#NOTE THAT I LOAD A CLUSTER MODULE, ELSE REPEATMASKER DOES NOT LOAD A COMPILER DEPENDENCY
rule repeatmasker:
    input:
        fasta = repeatmasker_input
    params:
        repeatmaskerpath = config["repeatmaskerpath"],
        species = config["species"]
    output:
        rmout = "aref/A.REF_repeatmasker/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa.out"
    resources:
        cpus_per_task =20,
        runtime = 1200,
        mem_mb = 50000
    conda:
        "omics"
    shell:
        """
module load libnsl
mkdir -p $(dirname {output})
{params.repeatmaskerpath} -species {params.species} -pa {resources.cpus_per_task} -gff {input.fasta} -dir $(dirname {output})
        """

rule getGtfs:
    input:
        rmout = "aref/A.REF_repeatmasker/{sample_or_ref}-pre-ins-filtering_nonrefcontigs.fa.out",
        rmref = config["starting_ref_repeatmasker"]
    params:
        module_prefix = config["prefix"]
    output:
       ref = "aref/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_ref_raw.gtf",
       nonref = "aref/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_nonref_raw.gtf",
       merged = "aref/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_raw.gtf"
    conda:
        "evo2"
    shell:
        """
workflow/{params.module_prefix}/scripts/outToGtf.sh {input.rmout} {output.nonref}
cp {input.rmref} {output.ref}
cat {output.ref} {output.nonref} > {output.merged}
        """

rule process_gtf_tldr:
    input:
        gtf = "aref/A.REF_repeatmasker/{sample_or_ref}_repeatmasker_raw.gtf",
        ref_cytobands = "aref/A.REF_annotations/cytobands.bed",
        ref = "aref/{sample_or_ref}-pre-ins-filtering.fa",
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt"
    output:
        contigs_to_keep = "aref/{sample_or_ref}_contigs_to_keep.txt",
        filtered_tldr = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.kept_in_updated_ref.txt",
        repmask_gff2 = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff2",
        repmask_gff3 = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gff3",
        r_annotation_fragmentsjoined = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv"
    conda:
        "evo2"
    script:
        "scripts/process_gtf_tldr.R"

rule analyze_insertions:
    input:
        tldroutput = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt",
        filtered_tldr = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.kept_in_updated_ref.txt",
        r_annotation_fragmentsjoined = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/{sample_or_ref}-pre-ins-filtering.fa"
    output:
        plots = "aref/{sample_or_ref}_Analysis/tldr_plots/tldr_plots.rds",
    conda:
        "repeatanalysis"
    script:
        "scripts/analyze_insertions.R"
        
rule cleanup_updated_ref:
    input:
        updated_reference = "aref/A.REF-pre-ins-filtering.fa",
        contigs_to_keep = "aref/A.REF_contigs_to_keep.txt"
    output:
        filtered_ref = "aref/A.REF.fa"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}
samtools faidx {output.filtered_ref}
        """

rule cleanup_updated_sample_ref:
    input:
        updated_reference = "aref/{sample}-pre-ins-filtering.fa",
        contigs_to_keep = "aref/{sample}_contigs_to_keep.txt"
    output:
        filtered_ref = "aref/{sample}.fa"
    wildcard_constraints:
        sample = "[A-Za-z0-9_]+"
    conda:
        "omics"
    shell:
        """
seqkit grep --by-name -f {input.contigs_to_keep} {input.updated_reference} > {output.filtered_ref}
samtools faidx {output.filtered_ref}
        """

rule process_gtf:
    input:
        gtf = "aref/A.REF_repeatmasker/A.REF_repeatmasker_raw.gtf",
        ref_cytobands = "aref/A.REF_annotations/cytobands.bed",
        ref = "aref/A.REF.fa"
    output:
        repmask_gff2 = "aref/A.REF_annotations/A.REF_repeatmasker.gff2",
        repmask_gff3 = "aref/A.REF_annotations/A.REF_repeatmasker.gff3",
        r_annotation_fragmentsjoined = "aref/A.REF_annotations/A.REF_repeatmasker.gtf.rformatted.fragmentsjoined.csv"
    conda:
        "evo2"
    script:
        "scripts/process_gtf.R"

rule move_refseq:
    input:
        ref_refseq_gff3 = config["ref_refseq_gff3"],
        ref_refseq_gtf = config["ref_refseq_gtf"],
        ref_refseq_bed = config["ref_refseq_bed"]
    output:
        refseq_gff3 = "aref/{sample_or_ref}_annotations/refseq.gff3",
        refseq_gtf = "aref/{sample_or_ref}_annotations/refseq.gtf",
        refseq_bed = "aref/{sample_or_ref}_annotations/refseq.bed"
    shell:
        """
cp {input.ref_refseq_gff3} {output.refseq_gff3}
cp {input.ref_refseq_gtf} {output.refseq_gtf}
cp {input.ref_refseq_bed} {output.refseq_bed}
        """


rule complete_gff3:
    input:
        gff3 = "aref/{sample_or_ref}_annotations/{annotation}.gff3",
    output:
        complete_gff3 = "aref/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
agat_convert_sp_gxf2gxf.pl --gff {input.gff3} -o {output.complete_gff3}.temp
awk '!/#/ {{print}}' {output.complete_gff3}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.complete_gff3}
rm  {output.complete_gff3}.temp
        """


rule gff_to_gtf:
    input:
        complete_gff3 = "aref/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    output:
        gtf = "aref/{sample_or_ref}_annotations/{annotation}.complete.gtf",
    conda:
        "omics"
    resources:
        mem_mb = 128000,
        runtime = 300
    shell:
        """
agat_convert_sp_gff2gtf.pl --gff {input.complete_gff3} -o {output.gtf}.temp
awk '!/#/ {{print}}' {output.gtf}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.gtf}
rm {output.gtf}.temp
        """

rule gff_to_bed12:
    input:
        complete_gff3 = "aref/{sample_or_ref}_annotations/{annotation}.complete.gff3",
    output:
        bed12 = "aref/{sample_or_ref}_annotations/{annotation}.complete.bed"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
agat_convert_sp_gff2bed.pl --gff {input.complete_gff3} -o {output.bed12}.temp
awk '!/#/ {{print}}' {output.bed12}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k2,2n -k3,3n > {output.bed12}
rm {output.bed12}.temp
        """

rule merge_genes_and_repeats_gff:
    input:
        complete_repeat_gff3 = "aref/A.REF_annotations/A.REF_repeatmasker.complete.gff3",
        complete_refseq_gff3 = "aref/A.REF_annotations/refseq.complete.gff3"
    output:
        merged_gff3 = "aref/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gff3"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
agat_sp_merge_annotations.pl -f {input.complete_refseq_gff3} -f {input.complete_repeat_gff3} -o {output.merged_gff3}.temp
awk '!/#/ {{print}}' {output.merged_gff3}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n >  {output.merged_gff3}
rm {output.merged_gff3}.temp
        """

rule merge_OG_genes_and_repeats_gff3:
    input:
        repeatmasker = "aref/A.REF_annotations/A.REF_repeatmasker.gff3",
        refseq = "aref/{sample_or_ref}_annotations/refseq.gff3"
    output:
        merged_gff3 = "aref/A.REF_annotations/A.REF_repeatmasker_refseq.gff3"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
awk '!/#/ {{print}}' {input.repeatmasker} | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' > {output.merged_gff3}.temp
awk '!/#/ {{print}}' {input.refseq} | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' >> {output.merged_gff3}.temp
cat {output.merged_gff3}.temp | sort -k1,1V -k4,4n -k5,5n > {output.merged_gff3}
rm {output.merged_gff3}.temp
        """


rule merge_genes_and_repeats_gtf:
    input:
        complete_repeat_gtf = "aref/A.REF_annotations/A.REF_repeatmasker.complete.gtf",
        complete_refseq_gtf = "aref/A.REF_annotations/refseq.complete.gtf"
    output:
        merged_gtf = "aref/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gtf"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
agat_sp_merge_annotations.pl -f {input.complete_refseq_gtf} -f {input.complete_repeat_gtf} -o {output.merged_gtf}.temp
awk '!/#/ {{print}}' {output.merged_gtf}.temp | awk '{{FS="\t";OFS="\t"}} $4 < 900000000000 {{print}}' | sort -k1,1V -k4,4n -k5,5n > {output.merged_gtf}
rm {output.merged_gtf}.temp
        """

rule tabixindex:
    input:
        annot = "aref/{sample_or_ref}_annotations/{annot}"
    output:
        gz = "aref/{sample_or_ref}_annotations/{annot}.gz",
        index = "aref/{sample_or_ref}_annotations/{annot}.gz.tbi"
    resources:
        mem_mb = 60000,
        runtime = 300
    conda:
        "omics"
    shell:
        """

awk '!/#/ {{print}}' {input.annot} | sort -k1,1V -k4,4n -k5,5n -t '\t'| bgzip > {input.annot}.gz
tabix -p gff {input.annot}.gz
        """

rule get2bitgenome:
    input:
        ref = "aref/A.REF.fa"
    output:
        genome2bit = "aref/A.REF.2bit"
    conda:
        "omics"
    shell:
        """
faToTwoBit {input.ref} {output.genome2bit}
        """


rule makeTxDB:
    input:
        refseq = "aref/A.REF_annotations/refseq.gff3",
        repeatmasker = "aref/A.REF_annotations/A.REF_repeatmasker.complete.gff3",
        genome2bit = "aref/A.REF.2bit"
    output:
        txdb = "aref/A.REF_annotations/A.REF_repeatmasker_refseq.complete.sqlite",
        txdbrefseq = "aref/A.REF_annotations/refseq.sqlite",
        txdbrepeatmasker = "aref/A.REF_annotations/A.REF_repeatmasker.complete.sqlite"
    resources:
        mem_mb = 40000
    conda:
        "repeatanalysis"
    script:
        "scripts/txdbBSgenome.R"

rule get_transcriptome:
    input:
        gtf = "aref/{sample_or_ref}_annotations/{annot}.gtf",
        ref = "aref/A.REF.fa"
    output:
        fa = "aref/{sample_or_ref}_annotations/{annot}.fa"
    resources:
        mem_mb = 128000,
        runtime = 300
    conda:
        "omics"
    shell:
        """
agat_sp_extract_sequences.pl -g {input.gtf} -f {input.ref} -t exon --merge -o {output.fa}
        """

rule annotate_rtes:
    input:
        r_annotation_fragmentsjoined = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        ref = "aref/{sample_or_ref}.fa",
        txdbrefseq = "aref/A.REF_annotations/refseq.sqlite"
    output:
        r_repeatmasker_annotation = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        rmann = "aref/{sample_or_ref}_annotations/{sample_or_ref}_rmann.csv",
        rmann_nonref = "aref/{sample_or_ref}_annotations/{sample_or_ref}_rmann_nonref.csv"
    conda:
        "evo2"
    script:
        "scripts/annotate_rtes.R"

rule getRTEbeds:
    input:
        r_annotation_fragmentsjoined = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
    output:
        outfile = "aref/{sample_or_ref}_annotations/{sample_or_ref}_rte_beds/outfile.txt"
    conda:
        "evo2"
    script:
        "scripts/getRTEbeds.R"

rule element_analysis:
    input:
        r_annotation_fragmentsjoined = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/{sample_or_ref}.fa"
    params:
        l13 = config["l13fasta"]
    output:
        outfile = "aref/{sample_or_ref}_Analysis/l1element_analysis.outfile",
        plots = "aref/{sample_or_ref}_Analysis/l1element_analysis.rds"
    conda:
        "evo2"
    script:
        "scripts/element_analysis.R"


rule make_star_index:
    input:
        reference = "aref/{sample_or_ref}.fa"
    output:
        outfile = "aref/{sample_or_ref}_indeces/make_star_index.out"
    resources:
        cpus_per_task = 32,
        mem_mb = 128000,
        runtime = 300
    conda:
        "star"
    shell:
        """
mkdir -p $(dirname {output.outfile})
STAR --runThreadN  30 --runMode genomeGenerate --genomeDir $(dirname {output.outfile})/star_index --genomeFastaFiles {input.reference}
touch {output.outfile}
        """

rule cpgIslandFun:
    params:
        ref_cpgislands = config["ref_cpgislands"]
    output:
        cpg_islands_fullinfo = "aref/A.REF_annotations/cpg_islands.tsv",
        cpg_islands = "aref/A.REF_annotations/cpg_islands.bed",
        cpgi_shores = "aref/A.REF_annotations/cpgi_shores.bed",
        cpgi_shelves = "aref/A.REF_annotations/cpgi_shelves.bed"
    resources:
        cpus_per_task = 2,
        mem_mb = 20000,
        runtime = 60
    conda:
        "ds"
    script:
        "scripts/cpgIslandFun.R"

rule copySelectAnnotations:
    params:
        ref_cytobands = config["ref_cytobands"],
        ref_telomere = config["ref_telomere"],
        ref_ccres = config["ref_ccres"],
        ref_clinvar_bgz = config["ref_clinvar_bgz"],
        ref_clinvar_tbi = config["ref_clinvar_tbi"]
    output:
        ref_cytobands = "aref/A.REF_annotations/cytobands.bed",
        ref_telomere = "aref/A.REF_annotations/telomeres.bed",
        ref_ccres = "aref/A.REF_annotations/ccres.bed",
        ref_clinvar_bgz = "aref/A.REF_annotations/clinvar.vcf.gz",
        ref_clinvar_tbi = "aref/A.REF_annotations/clinvar.vcf.gz.tbi"
    shell:
        """
cp {params.ref_cytobands} {output.ref_cytobands}
cp {params.ref_telomere} {output.ref_telomere}
cp {params.ref_ccres} {output.ref_ccres}
cp {params.ref_clinvar_bgz} {output.ref_clinvar_bgz}
cp {params.ref_clinvar_tbi} {output.ref_clinvar_tbi}
        """


rule create_blast_db:
    input:
        ref = "aref/{sample_or_ref}.fa",
    output:
        blast_njs = "aref/{sample_or_ref}.njs"
    conda:
        "repeatanalysis"
    shell:
        """
makeblastdb -in {input.ref} -dbtype nucl -out aref/{wildcards.sample_or_ref}
        """

rule transduction_mapping:
    input:
        filtered_tldr = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.kept_in_updated_ref.txt",
        unfiltered_tldr = "aref/{sample_or_ref}_tldr/{sample_or_ref}.table.txt",
        r_annotation_fragmentsjoined = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker.gtf.rformatted.fragmentsjoined.csv",
        r_repeatmasker_annotation = "aref/{sample_or_ref}_annotations/{sample_or_ref}_repeatmasker_annotation.csv",
        ref = "aref/{sample_or_ref}.fa",
        blast_njs = "aref/{sample_or_ref}.njs"
    params:
        sample_or_ref = lambda w: w.sample_or_ref
    output:
        plots = "aref/{sample_or_ref}_Analysis/tldr_plots/transduction_mapping.rds",
        circlize_phylogeny = "aref/{sample_or_ref}_Analysis/tldr_plots/circlize_phylogeny.png",
        # circlize_transduction = "aref/{sample_or_ref}_Analysis/tldr_plots/circlize_transduction.png",
        # transduction_df = "aref/{sample_or_ref}_Analysis/transduction_df.csv"
    conda:
        "evo2"
    script:
        "scripts/transduction_mapping.R"