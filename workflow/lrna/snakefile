import os
import pandas as pd
from pathlib import Path
samples = config["samples"]

#tag FILESTRUCTURE
paths = [
    "lrna/envs",
    "lrna/report",
    "lrna/qc",
    "lrna/scripts",
    "lrna/qc/{sample}",
    "lrna/rawdata/{sample}",
    "lrna/logs/{sample}",
    "lrna/intermediates/{sample}/methylation",
    "lrna/intermediates/{sample}/dorado",
    "lrna/intermediates/{sample}/fastqs",
    "lrna/intermediates/{sample}/counts",
    "lrna/intermediates/{sample}/counts/genome",
    "lrna/intermediates/{sample}/counts/transcriptome",
    "lrna/intermediates/{sample}/alignments",
    "lrna/intermediates/{sample}/alignments/genome",
    "lrna/intermediates/{sample}/alignments/transcriptome",
    "lrna/results/{sample}",
    "lrna/results/plots",
    "lrna/results/tables"
    ]
for path in paths:
    for sample in samples:
        os.makedirs(path.format(sample=sample), exist_ok = True)

#tag FUNCTIONS
import re
def inferLibraryType():
    try: 
        f = open("lrna/qc/library_type.txt")
        data = f.read()
        lines = re.findall('Fraction.*', data)
        pctMapped = [float(line.split(": ")[1]) for line in lines]
        if pctMapped[1] > 0.75:
            libraryType = "forward"
        elif pctMapped[2] > 0.75:
            libraryType = "reverse"
        else:
            libraryType = "unstranded"
        print("library type = ", libraryType)
        return libraryType
    except:
        print("library type = didn't run infer library type yet")
        return "didn't run infer library type yet"

def getFeatureCountsStrandParam():
    libraryType = inferLibraryType()
    if libraryType == "forward":
        strandParam = "1"
    elif libraryType == "reverse":
        strandParam = "2"
    elif libraryType == "unstranded":
        strandParam = "0"
    else:
        strandParam = "didn't run infer library type yet"
    return strandParam


def getTelescopeStrandParam():
    libraryType = inferLibraryType()
    if libraryType == "forward":
        if config["READ_TYPE"] == "PE":
            strandParam = "FR"
        elif config["READ_TYPE"] == "SE":
            strandParam = "F"
    elif libraryType == "reverse":
        if config["READ_TYPE"] == "PE":
            strandParam = "RF"
        elif config["READ_TYPE"] == "SE":
            strandParam = "R"
    elif libraryType == "unstranded":
        strandParam = "None"
    else:
        strandParam = "didn't run infer library type yet"
    return strandParam

# rule pod5tofast5:
#     input:
#         dir = "lrna/rawdata/{sample}"
#     output:
#         dir = directory("lrna/rawdata/fast5/{sample}")
#     resources:
#         cpus_per_task = 10,
#         runtime = 800,
#         mem_mb = 128000,
#         disk_mb = 1000000
#     shell:
#         """
# mkdir -p {output.dir}
# pod5 convert to_fast5 --recursive --output {output.dir} -t 10 -f {input.dir}
#         """


# rule guppy:
#     input:
#         dir = "lrna/rawdata/fast5/{sample}"
#     params:
#         guppy = config["guppy"],
#         guppy_config = config["guppy_config"]
#     output:
#         summary = "lrna/intermediates/{sample}/guppy/sequencing_summary.txt"
#     resources:
#         cpus_per_task =8,
#         runtime = 5760,
#         slurm_partition="gpu-he",
#         mem_mb = 128000,
#         slurm_extra="--time=96:00:00 --gres=gpu:2 --mail-type=ALL --mail-user=maxfield_kelsey@brown.edu"
#     shell:
#         """
# mkdir -p $(dirname {output.summary})
# {params.guppy} \
# -i {input.dir} \
# -s $(dirname {output.summary}) \
# -c {params.guppy_config} \
# -x 'auto' \
# --recursive
#         """

# rule mergeGuppyFastqs:
#     input:
#         summary = "lrna/intermediates/{sample}/guppy/sequencing_summary.txt"
#     output:
#         fq = "lrna/intermediates/{sample}/fastqs/guppy/{sample}.fq.gz"
#     resources:
#         cpus_per_task = 8,
#         mem_mb = 64000
#     shell:
#         """
# rm -f {output.fq}
# for file in $(find $(dirname {input.summary})/pass -name "*.fastq" -type f)
# do
# cat $file >> {wildcards.sample}_temp_fq.txt
# done
# gzip {wildcards.sample}_temp_fq.txt
# mv {wildcards.sample}_temp_fq.txt.gz > {output.fq}
#         """

# rule gunzipfq:
#     input:
#         fqcalls = "lrna/intermediates/{sample}/fastqs/{alignmenttype}/{sample}.fq.gz",
#     output:
#         fq = "lrna/intermediates/{sample}/fastqs/{alignmenttype}/{sample}.fq",
#     resources:
#         cpus_per_task = 4,
#         mem_mb = 32000
#     shell:
#         """
# cp {input.fqcalls} {input.fqcalls}.temp.gz
# gunzip {input.fqcalls}.temp.gz
# mv {input.fqcalls}.temp {output.fq}
#         """

# rule nanopolishindex:
#     input:
#         fq = "lrna/intermediates/{sample}/fastqs/guppy/{sample}.fq",
#         dir = "lrna/rawdata/fast5/{sample}"
#     params:
#         guppy = config["guppy"],
#         guppy_config = config["guppy_config"],
#         reference = config["reference"]
#     output:
#         outfile = "lrna/outfiles/nanopolishindex_{sample}.txt"
#     resources:
#         cpus_per_task = 8,
#         runtime = 600,
#         mem_mb = 64000,
#     conda: "nanopolish"
#     shell:
#         """
# nanopolish index -d {input.dir} {input.fq}
# touch {output.outfile}
#         """

# rule eventalign:
#     input:
#         fq = "lrna/intermediates/{sample}/fastqs/guppy/{sample}.fq",
#         bam = "lrna/intermediates/{sample}/alignments/genome/guppy/{sample}.sorted.bam",
#         nanopolishindex = "lrna/outfiles/nanopolishindex_{sample}.txt"
#     params:
#         reference = config["reference"]
#     output:
#         eventalign = "lrna/intermediates/{sample}/eventalign/guppy/reads-ref.eventalign.txt"
#     resources:
#         cpus_per_task = 32,
#         runtime = 5760,
#         mem_mb = 156000,
#         disk_mb = 10000000
#     conda: "nanopolish"
#     shell:
#         """
# mkdir -p $(dirname {output.eventalign})
# nanopolish eventalign \
#     --reads {input.fq} \
#     --bam {input.bam} \
#     --genome {params.reference} \
#     --signal-index \
#     --threads 30 \
#     --scale-events > {output.eventalign}
#         """

# rule m6a_dataprep:
#     input:
#         eventalign = "lrna/intermediates/{sample}/eventalign/guppy/reads-ref.eventalign.txt"
#     output:
#         json = "lrna/intermediates/{sample}/m6a/prep/data.json"
#     resources:
#         cpus_per_task = 32,
#         runtime = 2000,
#         mem_mb = 128000,
#     conda: "m6anet"
#     shell:
#         """
# mkdir -p $(dirname {output.json})
# m6anet dataprep \
# --eventalign {input.eventalign} \
# --out_dir $(dirname {output.json}) \
# --n_processes 20
#         """

# rule m6a_inference:
#     input:
#         json = "lrna/intermediates/{sample}/m6a/prep/data.json"
#     output:
#         csv = "lrna/intermediates/{sample}/m6a/results/data.indiv_proba.csv"
#     resources:
#         cpus_per_task = 32,
#         runtime = 2000,
#         mem_mb = 128000,
#     conda: "m6anet"
#     shell:
#         """
# m6anet inference \
# --input_dir $(dirname {input.json}) \
# --out_dir $(dirname {output.csv})  \
# --n_processes 20 \
# --num_iterations 1000
#         """

rule dorado:
    input:
        dir = "lrna/rawdata/{rate}/{sample}",
        aref = "aref.done.outfile"
    params:
        dorado = config["dorado"],
        reference = lambda wildcards: "aref/%s.fa"%(wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF.fa"
    output:
        protected("lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.bam")
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modification_string = "[0-9A-Za-z_-]+"
    resources:
        cpus_per_task =12,
        threads = 12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        slurm_extra="--time=48:00:00 --constraint=a6000 --gres=gpu:2"
    conda: "minimap2"
    shell:
        """
mkdir -p $(dirname {output})
mod_string=$(echo {wildcards.modification_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{input.dir} \
--recursive \
--verbose \
--estimate-poly-a \
--reference {params.reference} \
--mm2-preset "splice" > {output}
        """

rule dorado_fastq:
    input:
        dir = "lrna/rawdata/{rate}/{sample}",
        aref = "aref.done.outfile"
    params:
        dorado = config["dorado"],
        reference = lambda wildcards: "aref/%s.fa"%(wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF.fa"
    output:
        protected("lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.nopolya.unaligned.bam")
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modification_string = "[0-9A-Za-z_-]+"
    resources:
        cpus_per_task =12,
        threads = 12,
        slurm_partition="gpu-he",
        mem_mb = 128000,
        slurm_extra="--time=48:00:00 --constraint=a6000 --gres=gpu:2"
    conda: "minimap2"
    shell:
        """
mkdir -p $(dirname {output})
mod_string=$(echo {wildcards.modification_string} | tr "-" ",")

{params.dorado} \
basecaller \
{wildcards.type},$mod_string \
{input.dir} \
--recursive \
--verbose \
 > {output}
        """

rule call_dorado:
    input:
        expand("lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.bam", sample = samples, rate = config["rate"], type = config["type"], modification_string = config["modification_string"])

# rule isoquant:
#     input:
#         fqcalls = expand("lrna/intermediates/{sample}/fastqs/{{alignmenttype}}/{sample}.fq.gz", sample = samples)
#     output:
#         "lrna/intermediates/isoquantNEW/{alignmenttype}_done.out"
#     priority: 100
#     params:
#         reference = config["reference"],
#         rtes_genes_gtf = config["rtes_genes_gtf"]
#     conda: "isoquant"
#     resources:
#         cpus_per_task =32,
#         runtime = 4000,
#         mem_mb = 128000,
#     shell:
#         """
# isoquant.py -d nanopore --stranded forward --fastq {input.fqcalls} \
#  --reference {params.reference} --genedb {params.rtes_genes_gtf} \
#  --output $(dirname {output}) --threads 30
#  touch {output}
#         """

# rule isoquantresume:
#     input:
#         fqcalls = expand("lrna/intermediates/{sample}/fastqs/{{alignmenttype}}/{sample}.fq.gz", sample = samples)
#     output:
#         "lrna/intermediates/isoquantNEW/{alignmenttype}_done.resume.out"
#     priority: 100
#     conda: "isoquant"
#     resources:
#         cpus_per_task =32,
#         runtime = 4000,
#         mem_mb = 128000,
#     shell:
#         """
# isoquant.py --resume --output $(dirname {output}) --threads 30
# touch {output}
#         """

# rule fq_to_DNA_fasta_for_repeatmasker:
#     input:
#         fq = "lrna/intermediates/{sample}/fastqs/{alignmenttype}/{sample}.fq.gz"
#     output:
#         fa = "lrna/intermediates/{sample}/fastqs/{alignmenttype}/{sample}.fa"
#     resources:
#         cpus_per_task =20,
#         runtime = 900,
#         mem_mb = 80000
#     conda:
#         "omics"
#     shell:
#         """
# seqkit fq2fa {input.fq} > {output.fa}.temp
# seqkit seq --rna2dna {output.fa}.temp > {output.fa}
# rm {output.fa}.temp
#         """

# def inputToMergeGTFs(wildcards):
#     checkpoint_output = checkpoints.split_reads_fasta.get(**wildcards).output[0]
#     sample = wildcards.sample
#     alignmenttype = wildcards.alignmenttype
#     part_nums=glob_wildcards(os.path.join(checkpoint_output, "%s.part_{part_num}.fa"%sample)).part_num
#     expand_call = expand("lrna/intermediates/{{wildcards.sample}}/fastqs/repeatmasker/{{wildcards.alignmenttype}}/{part_num}/{{wildcards.sample}}.part_{part_num}.fa.gtf",part_num = part_nums)
#     list_call = ["lrna/intermediates/%s/repeatmasker/%s/%s/%s.part_%s.fa.gtf"%(sample, alignmenttype, part_num, sample, part_num) for part_num in part_nums]
#     return(list_call)


# checkpoint split_reads_fasta:
#     input:
#         fa = "lrna/intermediates/{sample}/fastqs/{alignmenttype}/{sample}.fa"
#     output:
#         directory("lrna/intermediates/{sample}/fastqs/{alignmenttype}/split")
#     conda:
#         "omics"
#     shell:
#         """
# mkdir -p {output}
# seqkit split2 {input.fa} -p 4 -f --out-dir {output}
#         """

# rule repeatmasker:
#     input:
#         chr_fasta = "lrna/intermediates/{sample}/fastqs/{alignmenttype}/split/{sample}.part_{part_num}.fa"
#     output:
#         rmout = "lrna/intermediates/{sample}/repeatmasker/{alignmenttype}/{part_num}/{sample}.part_{part_num}.fa.out"
#     resources:
#         cpus_per_task =20,
#         runtime = 1200,
#         mem_mb = 50000
#     conda:
#         "omics"
#     shell:
#         """
# mkdir -p $(dirname {output})
# /oscar/data/jsedivy/mkelsey/tools/RepeatMasker/RepeatMasker -species human -pa {resources.cpus_per_task} -gff {input.chr_fasta} -dir $(dirname {output})
#         """


# rule getGtfs:
#     input:
#         rmout = "lrna/intermediates/{sample}/repeatmasker/{alignmenttype}/{part_num}/{sample}.part_{part_num}.fa.out"
#     output:
#         gtfout = "lrna/intermediates/{sample}/repeatmasker/{alignmenttype}/{part_num}/{sample}.part_{part_num}.fa.gtf"
#     conda:
#         "evo2"
#     shell:
#         """
# scripts/outToGtf.sh {input.rmout} {output.gtfout}
#         """

# rule mergeGtfsandCleanupRM:
#     input:
#         inputToMergeGTFs
#     output:
#         gtf = "lrna/intermediates/{sample}/repeatmasker/{alignmenttype}/{sample}_repeatmasker.gtf"
#     conda:
#         "evo2"
#     shell:
#         """
# cat {input} > {output.gtf}
# sort -k1,1V -k4,4n -k5,5n {output.gtf} > tmp.gtf
# mv tmp.gtf {output.gtf}
# find aref/ -type d -name 'RM_*' -exec rm -r {} +
#         """

# rule analyzeRepeatMaskedReads:
#     input:
#         gtf = "lrna/intermediates/{sample}/repeatmasker/{alignmenttype}/{sample}_repeatmasker.gtf"
#     output:
#         plot = 
#     conda:
#         "repeatanalysis"
#     script:
#         "scripts/maskedReadAnalysis.R"

# rule minimap2RNAGENOME:
#     input:
#         fq = "lrna/intermediates/{sample}/fastqs/{alignmenttype}/{sample}.fq.gz"
#     params:
#         junctionbed = config["junctionbed"],
#         reference = lambda wildcards: "aref/%s.fa"%(wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF.fa"
#     output:
#         bam = "lrna/intermediates/{sample}/alignments/{sample}.sorted.bam"
#     resources:
#         cpus_per_task =20,
#         runtime = 900,
#         mem_mb = 80000
#     conda:
#         "minimap2"
#     shell:
#         """
# minimap2 -ax splice -uf -N 10 -k14 -t 12 --junc-bed {params.junctionbed} {params.reference} {input.fq} | \
# samtools sort -@4 -T {wildcards.sample} -O bam -o {output.bam}
# samtools index -@8 {output.bam}
# samtools stats {output.bam} > {output.bam}.stats.txt        
#         """


# rule minimap2RNATRANSCRIPTOME:
#     input:
#         fq = "lrna/intermediates/{sample}/fastqs/{sample}.fq.gz"
#     params:
#         referencetranscriptome = config["referencetranscriptome"]
#     output:
#         bam = "lrna/intermediates/{sample}/alignments/transcriptome/{sample}.sorted.bam"
#     resources:
#         cpus_per_task =20,
#         runtime = 900,
#         mem_mb = 80000
#     conda:
#         "minimap2"
#     shell:
#         """
# minimap2 -ax map-ont -uf -N 10 -k14 -t 12  {params.referencetranscriptome} {input.fq} | \
# samtools sort -@4 -T {wildcards.sample} -O bam -o {output.bam}
# samtools index -@8 {output.bam}
# samtools stats {output.bam} > {output.bam}.stats.txt        
#         """

# rule NanoCountgenesandRTEs:
#     input:
#         sortedbam = "lrna/intermediates/{sample}/alignments/transcriptome/{sample}.sorted.bam"
#     output:
#         counts = "lrna/intermediates/{sample}/counts/transcriptome/{sample}.nanocount.tsv",
#     conda:
#         "omics"
#     resources:
#         cpus_per_task =4,
#         runtime = 3000,
#         mem_mb = 32000,
#     shell: 
#         """
# NanoCount -i {input.sortedbam} -o {output.counts} --extra_tx_info
#         """

rule fc_relaxed:
    input:
        sortedbam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.good.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        libtype = "lrna/qc/library_type.txt"
    output:
        countsmessy = "lrna/intermediates/{sample}/counts/relaxed/{sample}rtesandgenes.counts_messy.txt",
        counts = "lrna/intermediates/{sample}/counts/relaxed/{sample}rtesandgenes.counts.txt",
    params: 
        featureCountsstrandparam = getFeatureCountsStrandParam(),
        annotation_genesandrtes = lambda wildcards: "aref/%s_annotations/%s_repeatmasker_refseq.complete.gtf"%(wildcards.sample, wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gtf"
    conda:
        "subread"
    resources:
        cpus_per_task =4,
        runtime = 3000,
        mem_mb = 100000,
    shell: 
        """
echo {params.featureCountsstrandparam}
mkdir -p $(dirname {output.counts})
featureCounts -s {params.featureCountsstrandparam} -L -O -M --primary --ignoreDup --largestOverlap -R CORE -a {params.annotation_genesandrtes} -o {output.countsmessy} {input.sortedbam}
cut -f1,7- {output.countsmessy} | awk 'NR > 1' > {output.counts}
        """

rule fc_relaxed_unstranded:
    input:
        sortedbam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.good.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        libtype = "lrna/qc/library_type.txt"
    output:
        countsmessy = "lrna/intermediates/{sample}/counts/relaxed_unstranded/{sample}rtesandgenes.counts_messy.txt",
        counts = "lrna/intermediates/{sample}/counts/relaxed_unstranded/{sample}rtesandgenes.counts.txt",
    params: 
        annotation_genesandrtes = lambda wildcards: "aref/%s_annotations/%s_repeatmasker_refseq.complete.gtf"%(wildcards.sample, wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gtf"
    conda:
        "subread"
    resources:
        cpus_per_task =4,
        runtime = 3000,
        mem_mb = 100000,
    shell: 
        """
echo "unstranded"
mkdir -p $(dirname {output.counts})
featureCounts -s 0 -L -O -M --primary --ignoreDup --largestOverlap -R CORE -a {params.annotation_genesandrtes} -o {output.countsmessy} {input.sortedbam}
cut -f1,7- {output.countsmessy} | awk 'NR > 1' > {output.counts}
        """

rule fc_relaxedunstrandedmapped:
    input:
        sortedbam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.good.mapped.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        libtype = "lrna/qc/library_type.txt"
    output:
        countsmessy = "lrna/intermediates/{sample}/counts/relaxedunstrandedmapped/{sample}rtesandgenes.counts_messy.txt",
        counts = "lrna/intermediates/{sample}/counts/relaxedunstrandedmapped/{sample}rtesandgenes.counts.txt",
    params: 
        annotation_genesandrtes = lambda wildcards: "aref/%s_annotations/%s_repeatmasker_refseq.complete.gtf"%(wildcards.sample, wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gtf"
    conda:
        "subread"
    resources:
        cpus_per_task =4,
        runtime = 3000,
        mem_mb = 100000,
    shell: 
        """
echo "unstranded"
mkdir -p $(dirname {output.counts})
featureCounts -s 0 -L -O -M --primary --ignoreDup --largestOverlap -R CORE -a {params.annotation_genesandrtes} -o {output.countsmessy} {input.sortedbam}
cut -f1,7- {output.countsmessy} | awk 'NR > 1' > {output.counts}
        """

rule fc_stringent:
    input:
        sortedbam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.good.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        libtype = "lrna/qc/library_type.txt"
    output:
        countsmessy = "lrna/intermediates/{sample}/counts/stringent/{sample}rtesandgenes.counts_messy.txt",
        counts = "lrna/intermediates/{sample}/counts/stringent/{sample}rtesandgenes.counts.txt",
    params: 
        featureCountsstrandparam = getFeatureCountsStrandParam(),
        annotation_genesandrtes = lambda wildcards: "aref/%s_annotations/%s_repeatmasker_refseq.complete.gtf"%(wildcards.sample, wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF_annotations/A.REF_repeatmasker_refseq.complete.gtf"
    conda:
        "omics"
    resources:
        cpus_per_task =10,
        runtime = 3000,
        mem_mb = 100000,
    shell: 
        """
echo {params.featureCountsstrandparam}
mkdir -p $(dirname {output.counts})
featureCounts -s {params.featureCountsstrandparam} -L -O -M --primary --ignoreDup --largestOverlap --fracOverlapFeature 0.5 --fracOverlap 0.5 -R CORE -a {params.annotation_genesandrtes} -o {output.countsmessy} {input.sortedbam}
cut -f1,7- {output.countsmessy} | awk 'NR > 1' > {output.counts}
        """

rule deseq:
    input:
        counts = expand("lrna/intermediates/{sample}/counts/{{counttype}}/{sample}rtesandgenes.counts.txt", sample = samples),
        unstranded = expand("lrna/intermediates/{sample}/counts/relaxed_unstranded/{sample}rtesandgenes.counts.txt", sample = samples)
    params:
        sample_table = config["sample_table"],
        contrasts = config["contrasts"],
        levels = config["levels"],
        paralellize_bioc = config["paralellize_bioc"],
        counttype = lambda w: w.counttype,
        outputdir = lambda w: "results/agg/deseq/%s"%(w.counttype)
    resources:
        cpus_per_task =10,
        mem_mb = 200000,
        runtime = 1000
    conda: "deseq"
    wildcard_constraints:
        counttype="[A-Za-z0-9]+"
    output:
        results_genes = expand("lrna/results/agg/deseq/{{counttype}}/{contrast}/results_genes.csv", contrast = config["contrasts"]),
        results_rtes = expand("lrna/results/agg/deseq/{{counttype}}/{contrast}/results_rtes.csv", contrast = config["contrasts"]),
        counts_normed = "lrna/results/agg/deseq/{counttype}/counttablesizenormed.csv",
        sizefactors = "lrna/results/agg/deseq/{counttype}/sizefactors.csv",
        plots = "lrna/results/agg/deseq/{counttype}/deseq_plots.RData"
    script:
        "scripts/deseq.R"

rule consolidateDeseqResults:
    input:
        results = expand("lrna/results/agg/deseq/{{counttype}}/{contrast}/results_rtes.csv", contrast = config["contrasts"], counttype = config["counttypes"]),
        counts_normed = expand("lrna/results/agg/deseq/{{counttype}}/counttablesizenormed.csv", counttype = config["counttypes"])
    params:
        inputdir =lambda w, input: os.path.dirname(os.path.dirname(input.counts_normed[0])),
    conda:
        "repeatanalysis"
    resources:
        cpus_per_task =10,
        mem_mb = 164000,
        runtime = 300
    output:
        resultsdf = "lrna/results/agg/deseq/{counttype}/resultsdf.tsv"
    script:
        "scripts/consolidateDeseqResults.R"

#tag ENRICHMENT ANALYSIS
import os
rule enrichment_analysis:
    input:
        resultsdf = "lrna/results/agg/deseq/{counttype}/resultsdf.tsv"
    params:
        inputdir =lambda w, input: os.path.dirname(os.path.dirname(input.resultsdf[0])),
        contrasts = config["contrasts"],
        genesets_for_heatmaps = config["genesets_for_heatmaps"],
        genesets_for_gsea = config["genesets_for_gsea"],
        sample_table = config["sample_table"],
        outputdir = lambda w, output: os.path.dirname(output.outfile)
    conda:
        "ea"
    resources:
        cpus_per_task =10,
        mem_mb = 164000,
        runtime = 300
    output:
        plots = "lrna/results/agg/enrichment_analysis/{counttype}/enrichment_analysis_plots.RData"
    script:
        "scripts/ea.R"

rule enrichment_analysis_repeats:
    input:
        resultsdf = "lrna/results/agg/deseq/{counttype}/resultsdf.tsv"
    params:
        inputdir =lambda w, input: os.path.dirname(os.path.dirname(input.resultsdf[0])),
        r_annotation_fragmentsjoined = config["r_annotation_fragmentsjoined"],
        r_repeatmasker_annotation = config["r_repeatmasker_annotation"],
        contrasts = config["contrasts"],
        sample_table = config["sample_table"],
        outputdir = lambda w, output: os.path.dirname(output.outfile)
    conda:
        "ea"
    resources:
        cpus_per_task =10,
        mem_mb = 164000,
        runtime = 300
    output:
        plots = "lrna/results/agg/enrichment_analysis/{counttype}/enrichment_analysis_repeats_plots.RData"

    script:
        "scripts/ea_repeats.R"


#tag REPETITIVE ELEMENTS
rule repeatanalysis_plots:
    input:
        resultsdf = "lrna/results/agg/deseq/{counttype}/resultsdf.tsv"
    params:
        r_annotation_fragmentsjoined = config["r_annotation_fragmentsjoined"],
        r_repeatmasker_annotation = config["r_repeatmasker_annotation"],
        contrasts = config["contrasts"],
        tecounttype = lambda w: w.counttype,
        outputdir = lambda w, output: os.path.dirname(output.outfile)
    conda:
        "repeatanalysis"
    resources:
        cpus_per_task =10,
        mem_mb = 164000,
        runtime = 300
    output:
        plots = "lrna/results/agg/repeatanalysis/{counttype}/repeatanalysis_plots.RData"
    script:
        "scripts/repeatanalysisPlots.R"




def bigwig_sizefactor_input(wildcards):
    sizefactor_path = "srna/results/agg/deseq_telescope/%s/sizefactors.csv"%(wildcards.tecounttype)
    if not Path(sizefactor_path).exists():
        return "NA"
    else:
        return 1/float(pd.read_csv("srna/results/agg/deseq_telescope/%s/sizefactors.csv"%(wildcards.tecounttype)).set_index("sample_name").loc[wildcards.sample, "sizefactor"])

# rule getBigWigF:
#     input:
#         sortedbam = "srna/outs/{sample}/star_output/{sample}.sorted.primary.bam",
#         sizefactors = "srna/results/agg/deseq_telescope/{tecounttype}/sizefactors.csv",
#     params:
#         sample_size_factor = bigwig_sizefactor_input
#     output:
#         bwF = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.F.bw",
#     resources:
#         cpus_per_task =10,
#         mem_mb = 40000
#     conda:
#         "deeptools"
#     shell: "bamCoverage -b {input.sortedbam} -o {output.bwF} --numberOfProcessors max --scaleFactor {params.sample_size_factor} --samFlagExclude 256 --filterRNAstrand forward --binSize 50"
#The --filterRNAstrand option assumes the sequencing library generated from ILLUMINA
# dUTP/NSR/NNSR methods, which are the most commonly used method for library preparation, 
#where Read 2 (R2) is in the direction of RNA strand (reverse-stranded library). 
#However other methods exist, which generate read R1 in the direction of RNA strand 
#(see this review). For these libraries, --filterRNAstrand will have an opposite behavior, 
#i.e. --filterRNAstrand forward will give you reverse strand signal and vice-versa.
#https://www.biostars.org/p/413626/#414440 useful thread about scaling factors

# rule getBigWigR:
#     input:
#         sortedbam = "srna/outs/{sample}/star_output/{sample}.sorted.primary.bam",
#         sizefactors = "srna/results/agg/deseq_telescope/relaxed/sizefactors.csv",
#     params:
#         sample_size_factor = bigwig_sizefactor_input
#     output:
#         bwR = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.R.bw",
#     resources:
#         cpus_per_task =10,
#         mem_mb = 40000
#     conda:
#         "deeptools"
#     shell: "bamCoverage -b {input.sortedbam} -o {output.bwR} --numberOfProcessors max --scaleFactor {params.sample_size_factor} --samFlagExclude 256 --filterRNAstrand reverse --binSize 50"
#The --filterRNAstrand option assumes the sequencing library generated from ILLUMINA
# dUTP/NSR/NNSR methods, which are the most commonly used method for library preparation, 
#where Read 2 (R2) is in the direction of RNA strand (reverse-stranded library). 
#However other methods exist, which generate read R1 in the direction of RNA strand 
#(see this review). For these libraries, --filterRNAstrand will have an opposite behavior, 
#i.e. --filterRNAstrand forward will give you reverse strand signal and vice-versa.
#https://www.biostars.org/p/413626/#414440 useful thread about scaling factors


# rule bigwigplots:
#     input:
#         bwF = expand("srna/outs/{sample}/star_output/{sample}.{counttype}.F.bw", sample = samples, counttype = config["counttypes"]),
#         bwR = expand("srna/outs/{sample}/star_output/{sample}.{counttype}.R.bw", sample = samples, counttype = config["counttypes"]),
#     params:
#         r_annotation_fragmentsjoined = config["r_annotation_fragmentsjoined"],
#         r_repeatmasker_annotation = config["r_repeatmasker_annotation"],
#         txdbrefseq = "aref/A.REF_annotations/refseq.sqlite"
#     output:
#         plots = "srna/results/agg/bigwig_plots/plots.rds"
#     resources:
#         cpus_per_task =10,
#         mem_mb = 40000
#     conda:
#         "repeatanalysis"
#     script: "scripts/bigwig_analysis.R"
    
#tag ALIGNMENT TOOLS
rule sortbam:
    input:
        bam = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.bam"
    wildcard_constraints:
        sample="[0-9A-Za-z_]+",
        type = "[0-9A-Za-z]+",
        rate = "[0-9A-Za-z]+",
        modification_string = "[0-9A-Za-z_-]+"    
    output:
        sortedbam = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam",
        sortedbambai = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam.bai",
        stats = "lrna/qc/{sample}/{rate}/{sample}.{type}.{modification_string}.sorted.bam.stats.txt"
    resources:
        cpus_per_task =10,
        mem_mb = 128000
    conda:
        "omics"
    shell: 
        """
samtools sort -@8 -m4g {input.bam} > {output.sortedbam}
samtools index  -@6 {output.sortedbam}
samtools stats {output.sortedbam} > {output.stats}
        """
    
rule filterbam:
    input:
        bam = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam",
    output:
        bam = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.filtered.bam",
        bai = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.filtered.bam.bai",
        stats = "lrna/qc/{sample}/{rate}/{sample}.{type}.{modification_string}.filtered.bam.stats.txt"
    resources:
        cpus_per_task =6,
        mem_mb = 60000
    conda:
        "omics"
    shell: "samtools view -b -F 0x100 -q 1 {input.bam} > {output.bam}; samtools index -@4 {output.bam}; samtools stats {output.bam} > {output.bam}.stats.txt"

rule mod_bam_to_fastq:
    input:
        sortedbam = "lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.nopolya.unaligned.bam",
    output:
        fastq = "lrna/intermediates/{sample}/fastqs/{rate}/{sample}.{type}.{modification_string}.fq.gz",
    conda: 
        "omics"
    resources:
        cpus_per_task =12
    shell:
        """
mkdir -p $(dirname {output.fastq})
samtools fastq -TMM,ML {input.sortedbam} > {input.sortedbam}.fq
gzip {input.sortedbam}.fq
mv {input.sortedbam}.fq.gz {output.fastq}
        """

rule realign:
    input:
        fastq = expand("lrna/intermediates/{{sample}}/fastqs/{rate}/{{sample}}.{type}.{modification_string}.fq.gz", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    params:
        reference = lambda wildcards: "aref/%s.fa"%(wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF.fa"
    output:
        bam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.good.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    resources:
        cpus_per_task =20,
        runtime = 900,
        mem_mb = 80000
    conda:
        "minimap2"
    shell:
        """
minimap2 -a -x splice -t 12 {params.reference} {input.fastq} | \
samtools sort -@4 -T {wildcards.sample} -O bam -o {output.bam}
samtools index -@8 {output.bam}
samtools stats {output.bam} > {output.bam}.stats.txt        
        """
rule realign_only_mapped:
    input:
        fastq = expand("lrna/intermediates/{{sample}}/fastqs/{rate}/{{sample}}.{type}.{modification_string}.fq.gz", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    params:
        reference = lambda wildcards: "aref/%s.fa"%(wildcards.sample) if config["per_sample_ref"] == "yes" else "aref/A.REF.fa"
    output:
        bam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.good.mapped.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    resources:
        cpus_per_task =20,
        runtime = 900,
        mem_mb = 80000
    conda:
        "minimap2"
    shell:
        """
minimap2 -a -x splice --sam-hit-only -t 12 {params.reference} {input.fastq} | \
samtools sort -@4 -T {wildcards.sample} -O bam -o {output.bam}
samtools index -@8 {output.bam}
samtools stats {output.bam} > {output.bam}.stats.txt        
        """

#tag QC
rule mycoplasmaCheck:
    input:
        fastq = expand("lrna/intermediates/{{sample}}/fastqs/{rate}/{{sample}}.{type}.{modification_string}.fq.gz", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    params:
        reference = config["mycoplasma"]
    output:
        bam = "lrna/qc/mycoplasma/mycoplasma{sample}.bam"
    resources:
        cpus_per_task =20,
        runtime = 900,
        mem_mb = 80000
    conda:
        "minimap2"
    shell:
        """
minimap2 -a -x map-ont -t 12 {params.reference} {input.fastq} | \
samtools sort -@4 -T {wildcards.sample} -O bam -o {output.bam}
samtools index -@8 {output.bam}
samtools stats {output.bam} > {output.bam}.stats.txt        
        """


rule dorado_seqsummary:
    input:
        sortedbam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"]),
        sortedbambai = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam.bai", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    params:
        dorado = config["dorado"]
    output:
        "lrna/qc/{sample}/{sample}.doradosummary.txt"
    conda:
        "omics"
    shell:
        """
{params.dorado} summary {input.sortedbam} > {output}
        """

rule pycoQC:
    input:
        seqsummary = "lrna/qc/{sample}/{sample}.doradosummary.txt",
        sortedbam = expand("lrna/intermediates/{{sample}}/alignments/{rate}/{{sample}}.{type}.{modification_string}.sorted.bam", rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    output:
        "lrna/qc/{sample}/{sample}pycoQC.html"
    conda:
        "pycoQC"
    shell:
        """
pycoQC --summary_file {input.seqsummary} --bam_file {input.sortedbam} --html_outfile {output} --min_pass_qual 10 --sample
        """


rule inferLibraryType:
    input:
        bam = expand("lrna/intermediates/{sample}/alignments/{rate}/{sample}.{type}.{modification_string}.sorted.bam", sample = samples[0], rate = config["rate"], type = config["type"], modification_string = config["modification_string"])
    output:
        librarytype = "lrna/qc/library_type.txt"
    params:
        gtf = config['annotation_genes_bed12'],
    resources:
        mem_mb  = 30000
    conda:
        "rseqc"
    shell:
        """
infer_experiment.py -r {params.gtf} -i {input.bam} > {output.librarytype}
        """

rule multiqc:
    input:
        counts_normed = expand("lrna/results/agg/deseq/{counttype}/counttablesizenormed.csv", counttype = config["counttypes"])     
    conda:
        "omics"
    output:
        report = "lrna/qc/multiqc_report.html",
    shell:
        """
multiqc --force --filename {output.report} --export --ignore "*guppy_basecaller_log*" .
        """


#tag NANOSIM
rule nanosimCharaceterization:
    input:
        reads = "lrna/intermediates/sen1/fastqs/sen1.fq.gz"
    output:
        outfile =  "lrna/nanosim/characterization/characterization/nanosimCharaceterization.out",
    params:
        reference = config["reference"],
        annotation = config["rtes_genes_gtf"],
        transcriptome = "/users/mkelsey/data/ref/generate/lf1/AnnotateReference/annotations/repeatmasker_refseq.complete.fa"
    resources:
        cpus_per_task =32,
        mem_mb = 128000
    conda:
        "nanosim"
    shell:
        """
mkdir -p $(dirname {output.outfile})
read_analysis.py transcriptome -rg {params.reference} -rt {params.transcriptome} -annot {params.annotation} -i {input.reads} -o $(dirname {output.outfile}) -t 20 -c
touch {output.outfile}
        """

rule nanosimSimulation:
    input:
        outfile =  "lrna/nanosim/characterization/characterization/nanosimCharaceterization.out"
    output:
        outfile =  "lrna/nanosim/nanosimSimulation.out",
        nanosimtranscriptome = "lrna/nanosim/l1hs_intact.compatiblewithnanosim.fa",
        reads = "lrna/nanosim/simulated_aligned_reads.fasta",
        readsPerfect = "lrna/nanosim/simulated_perfect_aligned_reads.fasta"
    params:
        reference = config["reference"],
        annotation = config["rtes_genes_gtf"],
        transcriptome = "/users/mkelsey/data/ref/generate/lf1/AnnotateReference/RefAnalysis/l1hs_intact.fa"
    resources:
        cpus_per_task =32,
        mem_mb = 128000
    conda:
        "nanosim"
    shell:
        """
cd nanosim
grep ">" {params.transcriptome} | sed 's/>//' | sed 's/_/-/g' | sed 's/\./-/g'  > l1hs_intact.fa.ids
cat {params.transcriptome} | sed 's/_/-/g' | sed 's/\./-/g' > l1hs_intact.compatiblewithnanosim.fa
num_seqs=$(wc -l {params.transcriptome}.ids | awk '{{print $1}}')
num_seqs=150
echo -e "target_id\ttest_counts\ttpm" > abundance.tsv
tpm=$(expr 1000000 / $num_seqs)
awk -v tpm=$tpm '{{print $1"\t"10"\t"tpm}}' l1hs_intact.fa.ids >> abundance.tsv
mkdir -p $(dirname {output.outfile})
simulator.py transcriptome -rt l1hs_intact.compatiblewithnanosim.fa -c characterization/characterization -e abundance.tsv -n 100000 -r dRNA --no_model_ir --seed 12 --output simulated
simulator.py transcriptome -rt l1hs_intact.compatiblewithnanosim.fa -c characterization/characterization -e abundance.tsv -n 100000 -r dRNA --no_model_ir --seed 12 --perfect --output simulated_perfect
cd ..
touch {output.outfile}
        """
#Underscores in the geneID throw it off!

rule nanosimAlignment:
    input:
        nanosimtranscriptome = "lrna/nanosim/l1hs_intact.compatiblewithnanosim.fa",
        reads = "lrna/nanosim/simulated_aligned_reads.fasta",
        readsPerfect = "lrna/nanosim/simulated_perfect_aligned_reads.fasta"
    params:
        reference = config["reference"],
        junctionbed = config["junctionbed"]
    output:
        bam =  "lrna/nanosim/nanosimAlignment.bam",
        bamp =  "lrna/nanosim/nanosimAlignment.perfect.bam",
        bamgenome =  "lrna/nanosim/nanosimAlignment_genome.bam",
        bampgenome =  "lrna/nanosim/nanosimAlignment_genome.perfect.bam"  
    resources:
        cpus_per_task =32,
        mem_mb = 128000
    conda:
        "minimap2"
    shell:
        """
minimap2 -ax map-ont -uf -N 10 -k14 -t 12 {input.nanosimtranscriptome} {input.reads} | samtools view -bS - | samtools sort -@ 12 -o {output.bam}
samtools index {output.bam}

minimap2 -ax map-ont -uf -N 10 -k14 -t 12 {input.nanosimtranscriptome} {input.readsPerfect} | samtools view -bS - | samtools sort -@ 12 -o {output.bamp}
samtools index {output.bamp}

minimap2 -ax map-ont -uf -N 10 -k14 -t 12 --junc-bed {params.junctionbed} {params.reference} {input.reads} | samtools view -bS - | samtools sort -@ 12 -o {output.bamgenome}
samtools index {output.bamgenome}

minimap2 -ax map-ont -uf -N 10 -k14 -t 12 --junc-bed {params.junctionbed} {params.reference} {input.readsPerfect} | samtools view -bS - | samtools sort -@ 12 -o {output.bampgenome}
samtools index {output.bampgenome}
        """

rule filterForPrimaryAlignments:
    input:
        bam = "lrna/nanosim/nanosimAlignment.bam",
        bamp = "lrna/nanosim/nanosimAlignment.perfect.bam",
        bamgenome = "lrna/nanosim/nanosimAlignment_genome.bam",
        bampgenome = "lrna/nanosim/nanosimAlignment_genome.perfect.bam"
    output:
        bam =  "lrna/nanosim/nanosimAlignment.primary.bam",
        bamp =  "lrna/nanosim/nanosimAlignment.perfect.primary.bam",
        bamgenome =  "lrna/nanosim/nanosimAlignment_genome.primary.bam",
        bampgenome =  "lrna/nanosim/nanosimAlignment_genome.perfect.primary.bam"
    threads: 4
    conda:
        "omics"
    shell: 
        """
samtools view -b -F 256 {input.bam} > {output.bam}
samtools index {output.bam}
samtools view -b -F 256 {input.bamp} > {output.bamp}
samtools index {output.bamp}
samtools view -b -F 256 {input.bamgenome} > {output.bamgenome}
samtools index {output.bamgenome}
samtools view -b -F 256 {input.bampgenome} > {output.bampgenome}
        """ 

rule analyzeNanosim:
    input:
        bamgenome =  "lrna/nanosim/nanosimAlignment_genome.primary.bam",
        bampgenome =  "lrna/nanosim/nanosimAlignment_genome.perfect.primary.bam"
    params:
        r_annotation_fragmentsjoined = config["r_annotation_fragmentsjoined"]
    output:
        plot = "lrna/nanosim/plots/mapping_accuracy_by_read_length.png"
    conda:
        "repeatanalysis"
    script: 
        "scripts/analyzeNanosim.R"

